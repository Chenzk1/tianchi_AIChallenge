{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm.plotting import plot_importance\n",
    "from lightgbm import LGBMRegressor\n",
    "from scipy import sparse\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('max_colwidth',200)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "BASE_PATH = os.path.join('../input')\n",
    "RAW_PATH = os.path.join(BASE_PATH, 'RAW_DATA')\n",
    "TRAIN_PATH = os.path.join(RAW_PATH, 'Metro_train')\n",
    "TEST_A_PATH = os.path.join(RAW_PATH, 'Metro_testA')\n",
    "SUBMIT_PATH = os.path.join('../submit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the file: record_2019-01-01.csv\n",
      "Memory usage of properties dataframe is : 135.62892150878906  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  70.23644256591797  MB\n",
      "This is  51.78574140720163 % of the initial size\n",
      "the file: record_2019-01-02.csv\n",
      "Memory usage of properties dataframe is : 126.91683959960938  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  65.72482872009277  MB\n",
      "This is  51.78574326892951 % of the initial size\n",
      "the file: record_2019-01-03.csv\n",
      "Memory usage of properties dataframe is : 122.4846420288086  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  63.42958354949951  MB\n",
      "This is  51.785744317708634 % of the initial size\n",
      "the file: record_2019-01-04.csv\n",
      "Memory usage of properties dataframe is : 129.44720458984375  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  67.03519630432129  MB\n",
      "This is  51.78574270238106 % of the initial size\n",
      "the file: record_2019-01-05.csv\n",
      "Memory usage of properties dataframe is : 107.7691650390625  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  55.80906867980957  MB\n",
      "This is  51.7857484184653 % of the initial size\n",
      "the file: record_2019-01-06.csv\n",
      "Memory usage of properties dataframe is : 104.04833221435547  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  53.882208824157715  MB\n",
      "This is  51.78574963907363 % of the initial size\n",
      "the file: record_2019-01-07.csv\n",
      "Memory usage of properties dataframe is : 120.56005859375  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  62.43292427062988  MB\n",
      "This is  51.78574479713009 % of the initial size\n",
      "the file: record_2019-01-08.csv\n",
      "Memory usage of properties dataframe is : 123.09117126464844  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  63.74367904663086  MB\n",
      "This is  51.7857441697266 % of the initial size\n",
      "the file: record_2019-01-09.csv\n",
      "Memory usage of properties dataframe is : 121.84334564208984  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  63.09748363494873  MB\n",
      "This is  51.785744475775616 % of the initial size\n",
      "the file: record_2019-01-10.csv\n",
      "Memory usage of properties dataframe is : 123.76958465576172  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  64.09500026702881  MB\n",
      "This is  51.78574400592453 % of the initial size\n",
      "the file: record_2019-01-11.csv\n",
      "Memory usage of properties dataframe is : 133.6138153076172  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  69.19290542602539  MB\n",
      "This is  51.78574181623625 % of the initial size\n",
      "the file: record_2019-01-12.csv\n",
      "Memory usage of properties dataframe is : 115.81388854980469  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  59.9750862121582  MB\n",
      "This is  51.78574604751871 % of the initial size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the file: record_2019-01-13.csv\n",
      "Memory usage of properties dataframe is : 110.20756530761719  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  57.07181167602539  MB\n",
      "This is  51.78574766326026 % of the initial size\n",
      "the file: record_2019-01-14.csv\n",
      "Memory usage of properties dataframe is : 128.0226058959961  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  66.29745769500732  MB\n",
      "This is  51.78574301859354 % of the initial size\n",
      "the file: record_2019-01-15.csv\n",
      "Memory usage of properties dataframe is : 123.16716766357422  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  63.783034324645996  MB\n",
      "This is  51.785744151287616 % of the initial size\n",
      "the file: record_2019-01-16.csv\n",
      "Memory usage of properties dataframe is : 129.08495330810547  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  66.84760189056396  MB\n",
      "This is  51.78574278212679 % of the initial size\n",
      "the file: record_2019-01-17.csv\n",
      "Memory usage of properties dataframe is : 132.1672134399414  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  68.443772315979  MB\n",
      "This is  51.785742117564425 % of the initial size\n",
      "the file: record_2019-01-18.csv\n",
      "Memory usage of properties dataframe is : 142.29454803466797  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.6882848739624  MB\n",
      "This is  51.785740136726346 % of the initial size\n",
      "the file: record_2019-01-19.csv\n",
      "Memory usage of properties dataframe is : 117.991455078125  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  61.10275459289551  MB\n",
      "This is  51.78574546134539 % of the initial size\n",
      "the file: record_2019-01-20.csv\n",
      "Memory usage of properties dataframe is : 112.88068389892578  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  58.45610523223877  MB\n",
      "This is  51.78574687284922 % of the initial size\n",
      "the file: record_2019-01-21.csv\n",
      "Memory usage of properties dataframe is : 131.69649505615234  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  68.20000743865967  MB\n",
      "This is  51.78574221704287 % of the initial size\n",
      "the file: record_2019-01-22.csv\n",
      "Memory usage of properties dataframe is : 132.2491912841797  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  68.48622512817383  MB\n",
      "This is  51.785742100312184 % of the initial size\n",
      "the file: record_2019-01-23.csv\n",
      "Memory usage of properties dataframe is : 134.794189453125  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  69.80417060852051  MB\n",
      "This is  51.785741575155264 % of the initial size\n",
      "the file: record_2019-01-24.csv\n",
      "Memory usage of properties dataframe is : 134.442138671875  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  69.62185859680176  MB\n",
      "This is  51.78574164661552 % of the initial size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the file: record_2019-01-25.csv\n",
      "Memory usage of properties dataframe is : 137.58303833007812  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  71.2483959197998  MB\n",
      "This is  51.785741021990226 % of the initial size\n",
      "Memory usage of properties dataframe is : 128.43527221679688  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  66.51115989685059  MB\n",
      "This is  51.785742926273954 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(props):\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if props[col].dtype != object:  # Exclude strings\n",
    "            \n",
    "            # Print current column type\n",
    "            print(\"******************************\")\n",
    "            print(\"Column: \",col)\n",
    "            print(\"dtype before: \",props[col].dtype)\n",
    "            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(props[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = props[col].fillna(0).astype(np.int64)\n",
    "            result = (props[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "            \n",
    "            # Print new column type\n",
    "            print(\"dtype after: \",props[col].dtype)\n",
    "            print(\"******************************\")\n",
    "    \n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return props\n",
    "def read_data(name, **params):\n",
    "    data = pd.read_csv(name, **params)\n",
    "    data = reduce_mem_usage(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "# ## 读取数据\n",
    "\n",
    "\n",
    "# payType-most\n",
    "# devideID-most two\n",
    "# userID-count(this stationID)-( nunique(userID) - nunique(userID)[payType==3])\n",
    "def get_hour_cut(data):\n",
    "    if data>= 23 or data <= 6:\n",
    "        hour_cut = 1\n",
    "    elif data>= 10 and data <= 13:\n",
    "        hour_cut = 2\n",
    "    elif data>= 18 and data <= 22:\n",
    "        hour_cut = 3\n",
    "    elif data>= 14 and data <= 17:\n",
    "        hour_cut = 4\n",
    "    else:\n",
    "        hour_cut = 5\n",
    "    return hour_cut\n",
    "def is_weekend(data):\n",
    "    if data <= 4:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "def date_processing(data):\n",
    "    data['startTime'] = data['time'].apply(lambda x: str(x)[:15]+ '0:00')\n",
    "    data['day'] = data['startTime'].apply(lambda x: int(str(x)[8:10]))\n",
    "    data['hour'] = data['startTime'].apply(lambda x: int(str(x)[11:13]))\n",
    "    data['minute'] = data['startTime'].apply(lambda x: int(str(x)[14:15]+'0'))# hour+10min 10min最后可以删除\n",
    "    data['startTime'] = pd.to_datetime(data['startTime'],format= '%Y-%m-%d %H:%M:%S')\n",
    "    data['weekday'] = data['startTime'].dt.weekday\n",
    "    #result['weekend'] = result['weekday'].apply(is_weekend)\n",
    "    \n",
    "    result = data.groupby(['stationID', 'startTime','day', 'hour', 'minute','weekday'])['status'].agg(['count','sum'])\n",
    "    result = result.reset_index()\n",
    "    result['inNums'] = result['sum']\n",
    "    result['outNums'] = result['count'] - result['sum']\n",
    "    \n",
    "    tmp     = data.groupby(['stationID'])['deviceID'].nunique().reset_index(name='nuni_deviceID_of_stationID')\n",
    "    result  = result.merge(tmp, on=['stationID'], how='left')\n",
    "    tmp     = data.groupby(['stationID','hour'])['deviceID'].nunique().reset_index(name='nuni_deviceID_of_stationID_hour')\n",
    "    result  = result.merge(tmp, on=['stationID','hour'], how='left')\n",
    "    tmp     = data.groupby(['stationID','hour','minute'])['deviceID'].nunique().                                           reset_index(name='nuni_deviceID_of_stationID_hour_minute')\n",
    "    result  = result.merge(tmp, on=['stationID','hour','minute'], how='left')\n",
    "    def get_top(df, n=1):\n",
    "        return df.sort_values()[-n:].values[0]\n",
    "    tmp     = data.groupby(['stationID'])['deviceID'].apply(get_top,n=1).reset_index(name='most_deviceID_of_stationID')\n",
    "    result  = result.merge(tmp, on=['stationID'], how='left')\n",
    "\n",
    "    tmp     = data.groupby(['stationID','hour'])['deviceID'].apply(get_top,n=1).reset_index(name='most_deviceID_of_stationID&hour')\n",
    "    result  = result.merge(tmp, on=['stationID','hour'], how='left')\n",
    "\n",
    "    tmp     = data.groupby(['stationID','weekday','hour'])['deviceID'].apply(get_top,n=1).reset_index(name='most_deviceID_of_stationID&wh')\n",
    "    result  = result.merge(tmp, on=['stationID','weekday','hour'], how='left')\n",
    "\n",
    "    tmp     = data.groupby(['stationID'])['payType'].apply(get_top,n=1).reset_index(name='most_payType_of_stationID')\n",
    "    result  = result.merge(tmp, on=['stationID'], how='left')\n",
    "    tmp     = data.groupby(['stationID','hour'])['payType'].apply(get_top,n=1).reset_index(name='most_payType_of_stationID&hour')\n",
    "    result  = result.merge(tmp, on=['stationID','hour'], how='left')\n",
    "    tmp     = data.groupby(['stationID','weekday','hour'])['payType'].apply(get_top,n=1).reset_index(name='most_payType_of_stationID&wh')\n",
    "    result  = result.merge(tmp, on=['stationID','weekday','hour'], how='left')\n",
    "\n",
    "    #result['weekday'] = result['startTime'].dt.weekday\n",
    "    result['hourCut'] = result['hour'].map(get_hour_cut)\n",
    "    result = result.drop(columns=['count', 'sum'])\n",
    "    # datetime -> int\n",
    "    return result\n",
    "def date_processing_test(data):\n",
    "    result = data\n",
    "    \n",
    "    result['day'] = result['startTime'].apply(lambda x: int(str(x)[8:10]))\n",
    "    result['startTime'] = pd.to_datetime(result['startTime'],format= '%Y-%m-%d %H:%M:%S')\n",
    "    result['weekday'] = result['startTime'].dt.weekday\n",
    "    #result['weekend'] = result['weekday'].apply(is_weekend)\n",
    "    result['hour'] = result['startTime'].apply(lambda x: int(str(x)[11:13]))\n",
    "    result['minute'] = result['startTime'].apply(lambda x: int(str(x)[14:15]+'0'))# hour+10min 10min最后可以删除\n",
    "    result['hourCut'] = result['hour'].map(get_hour_cut)\n",
    "    result = result.drop(columns='endTime')\n",
    "    result = result.drop(columns=['inNums', 'outNums'])\n",
    "    return result  \n",
    "\n",
    "## 读取数据\n",
    "data = pd.DataFrame()\n",
    "for file in os.listdir(TRAIN_PATH):\n",
    "    print(f'the file: {file}')\n",
    "    temp = read_data(os.path.join(TRAIN_PATH, file))\n",
    "    temp = date_processing(temp)\n",
    "    data = pd.concat([data, temp],ignore_index=True)\n",
    "    del temp\n",
    "test_name = os.path.join(TEST_A_PATH, 'testA_record_2019-01-28.csv')\n",
    "test_28 = read_data(test_name)\n",
    "test_28 = date_processing(test_28)\n",
    "data = pd.concat([data, test_28],ignore_index=True)\n",
    "test_name = os.path.join(TEST_A_PATH, 'testA_submit_2019-01-29.csv')\n",
    "test = pd.read_csv(test_name)\n",
    "test = date_processing_test(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/after_base_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stationID</th>\n",
       "      <th>startTime</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>weekday</th>\n",
       "      <th>inNums</th>\n",
       "      <th>outNums</th>\n",
       "      <th>nuni_deviceID_of_stationID</th>\n",
       "      <th>nuni_deviceID_of_stationID_hour</th>\n",
       "      <th>nuni_deviceID_of_stationID_hour_minute</th>\n",
       "      <th>nuni_deviceID_of_stationID_wh</th>\n",
       "      <th>nuni_deviceID_of_stationID_whm</th>\n",
       "      <th>most_deviceID_of_stationID</th>\n",
       "      <th>most_deviceID_of_stationID&amp;hour</th>\n",
       "      <th>most_deviceID_of_stationID&amp;wh</th>\n",
       "      <th>most_payType_of_stationID</th>\n",
       "      <th>most_payType_of_stationID&amp;hour</th>\n",
       "      <th>most_payType_of_stationID&amp;wh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-29 00:00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-29 00:10:00</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-29 00:20:00</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-29 00:30:00</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-29 00:40:00</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      stationID            startTime  day  hour  minute  weekday  inNums  \\\n",
       "2917          0  2019-01-29 00:00:00   21     0       0        1     0.0   \n",
       "2918          0  2019-01-29 00:10:00   21     0      10        1     0.0   \n",
       "2919          0  2019-01-29 00:20:00   21     0      20        1     0.0   \n",
       "2920          0  2019-01-29 00:30:00   21     0      30        1     0.0   \n",
       "2921          0  2019-01-29 00:40:00   21     0      40        1     0.0   \n",
       "\n",
       "      outNums  nuni_deviceID_of_stationID  nuni_deviceID_of_stationID_hour  \\\n",
       "2917      0.0                          18                               12   \n",
       "2918      0.0                          18                               12   \n",
       "2919      0.0                          18                               12   \n",
       "2920      0.0                          18                               12   \n",
       "2921      0.0                          18                               12   \n",
       "\n",
       "      nuni_deviceID_of_stationID_hour_minute  nuni_deviceID_of_stationID_wh  \\\n",
       "2917                                       7                              3   \n",
       "2918                                       6                              3   \n",
       "2919                                       6                              3   \n",
       "2920                                       5                              3   \n",
       "2921                                       5                              3   \n",
       "\n",
       "      nuni_deviceID_of_stationID_whm  most_deviceID_of_stationID  \\\n",
       "2917                               3                        17.0   \n",
       "2918                               1                        17.0   \n",
       "2919                               1                        17.0   \n",
       "2920                               1                        17.0   \n",
       "2921                               1                        17.0   \n",
       "\n",
       "      most_deviceID_of_stationID&hour  most_deviceID_of_stationID&wh  \\\n",
       "2917                             17.0                           15.0   \n",
       "2918                             17.0                           15.0   \n",
       "2919                             17.0                           15.0   \n",
       "2920                             17.0                           15.0   \n",
       "2921                             17.0                           15.0   \n",
       "\n",
       "      most_payType_of_stationID  most_payType_of_stationID&hour  \\\n",
       "2917                        3.0                             3.0   \n",
       "2918                        3.0                             3.0   \n",
       "2919                        3.0                             3.0   \n",
       "2920                        3.0                             3.0   \n",
       "2921                        3.0                             3.0   \n",
       "\n",
       "      most_payType_of_stationID&wh  \n",
       "2917                           3.0  \n",
       "2918                           3.0  \n",
       "2919                           3.0  \n",
       "2920                           3.0  \n",
       "2921                           3.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['day']==21].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## F E\n",
    "# 剔除周末,并修改为连续时间\n",
    "data = data[(data.day!=5)&(data.day!=6)]\n",
    "data = data[(data.day!=12)&(data.day!=13)]\n",
    "data = data[(data.day!=19)&(data.day!=20)]\n",
    "data = data[(data.day!=26)&(data.day!=27)]\n",
    "\n",
    "def fix_day(d):\n",
    "    if d in [1,2,3,4]:\n",
    "        return d\n",
    "    elif d in [7,8,9,10,11]:\n",
    "        return d - 2\n",
    "    elif d in [14,15,16,17,18]:\n",
    "        return d - 4\n",
    "    elif d in [21,22,23,24,25]:\n",
    "        return d - 6\n",
    "    elif d in [28,29]:\n",
    "        return d - 8\n",
    "data['day'] = data['day'].apply(fix_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc[data['day']==21,['inNums','outNums']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feat_count(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].count()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_count\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_nunique(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].nunique()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_nunique\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_mean(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].mean()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_mean\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_std(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].std()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_std\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_median(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].median()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_median\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_max(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].max()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_max\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_min(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].min()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_min\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_sum(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].sum()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_sum\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_var(df, df_feature, fe,value,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].var()).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_var\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def feat_quantile(df, df_feature, fe,value,n,name=\"\"):\n",
    "    df_count = pd.DataFrame(df_feature.groupby(fe)[value].quantile(n)).reset_index()\n",
    "    if not name:\n",
    "        df_count.columns = fe + [value+\"_%s_quantile\" % (\"_\".join(fe))]\n",
    "    else:\n",
    "        df_count.columns = fe + [name]\n",
    "    df = df.merge(df_count, on=fe, how=\"left\").fillna(0)\n",
    "    return df\n",
    "#['stationID', 'startTime', 'day', 'hour', 'minute', 'weekday', 'inNums', 'outNums',  'day_gap']\n",
    "def create_features(df_label, df_train):\n",
    "    # nums\n",
    "    #这里加入前一天的数据\n",
    "\n",
    "    for i in [1,3,5,10,15]:\n",
    "        if df_train.day_gap.min() > -i:\n",
    "            break\n",
    "        df_select=df_train[df_train.day_gap>=-i].copy()\n",
    "        if i==1:\n",
    "            df_label = feat_mean(df_label,df_select,[\"stationID\"],\"inNums\", \"inNums_mean_stationID_%s\"%i)\n",
    "            df_label = feat_mean(df_label,df_select,[\"stationID\"],\"outNums\", \"outNums_mean_stationID_%s\"%i)\n",
    "            df_label=feat_mean(df_label,df_select,[\"stationID\", 'hour'],\"inNums\", \"inNums_mean_s_h_%s\"%i)\n",
    "            df_label=feat_mean(df_label,df_select,[\"stationID\", 'hour'],\"outNums\", \"outNums_mean_s_h_%s\"%i)\n",
    "            df_label=feat_mean(df_label,df_select,[\"stationID\", 'hour','minute'],\"inNums\", \"inNums_mean_s_h_m_%s\"%i)\n",
    "            df_label=feat_mean(df_label,df_select,[\"stationID\", 'hour','minute'],\"outNums\", \"outNums_mean_s_h_m_%s\"%i)\n",
    "            continue        \n",
    "        # stationID\n",
    "        df_label=feat_mean(df_label,df_select,[\"stationID\"],\"inNums\", \"inNums_mean_stationID_%s\"%i)\n",
    "        df_label=feat_std(df_label,df_select,[\"stationID\"],\"inNums\", \"inNums_std_stationID_%s\"%i)\n",
    "        df_label=feat_median(df_label,df_select,[\"stationID\"],\"inNums\", \"inNums_median_stationID_%s\"%i)\n",
    "        df_label=feat_max(df_label,df_select,[\"stationID\"],\"inNums\", \"inNums_max_stationID_%s\"%i)\n",
    "        df_label=feat_min(df_label,df_select,[\"stationID\"],\"inNums\", \"inNums_min_stationID_%s\"%i)\n",
    "        df_label=feat_var(df_label,df_select,[\"stationID\"],\"inNums\", \"inNums_var_stationID_%s\"%i)\n",
    "        #df_label=feat_quantile(df_label,df_select,[\"stationID\"],\"inNums\", \"inNums_quantile_stationID_%s\"%i)\n",
    "\n",
    "        df_label=feat_mean(df_label,df_select,[\"stationID\"],\"outNums\", \"outNums_mean_stationID_%s\"%i)\n",
    "        df_label=feat_std(df_label,df_select,[\"stationID\"],\"outNums\", \"outNums_std_stationID_%s\"%i)\n",
    "        df_label=feat_median(df_label,df_select,[\"stationID\"],\"outNums\", \"outNums_median_stationID_%s\"%i)\n",
    "        df_label=feat_max(df_label,df_select,[\"stationID\"],\"outNums\", \"outNums_max_stationID_%s\"%i)\n",
    "        df_label=feat_min(df_label,df_select,[\"stationID\"],\"outNums\", \"outNums_min_stationID_%s\"%i)\n",
    "        df_label=feat_var(df_label,df_select,[\"stationID\"],\"outNums\", \"outNums_var_stationID_%s\"%i)\n",
    "        #df_label=feat_quantile(df_label,df_select,[\"stationID\"],\"outNums\", \"outNums_quantile_stationID_%s\"%i)\n",
    "       \n",
    "        # stationID weekday hour\n",
    "        df_label=feat_mean(df_label,df_select,[\"stationID\", 'weekday','hour'],\"inNums\", \"inNums_mean_s_w_h_%s\"%i)\n",
    "        df_label=feat_std(df_label,df_select,[\"stationID\", 'weekday','hour'],\"inNums\", \"inNums_std_s_w_h_%s\"%i)\n",
    "        df_label=feat_median(df_label,df_select,[\"stationID\", 'weekday','hour'],\"inNums\", \"inNums_median_s_w_h_%s\"%i)\n",
    "        df_label=feat_max(df_label,df_select,[\"stationID\", 'weekday','hour'],\"inNums\", \"inNums_max_s_w_h_%s\"%i)\n",
    "        df_label=feat_min(df_label,df_select,[\"stationID\", 'weekday','hour'],\"inNums\", \"inNums_min_s_w_h_%s\"%i)\n",
    "        df_label=feat_var(df_label,df_select,[\"stationID\", 'weekday','hour'],\"inNums\", \"inNums_var_s_w_h_%s\"%i)\n",
    "        #df_label=feat_quantile(df_label,df_select,[\"stationID\", 'weekday','hour'],\"inNums\", \"inNums_quantile_s_w_h_%s\"%i)\n",
    "\n",
    "        df_label=feat_mean(df_label,df_select,[\"stationID\", 'weekday','hour'],\"outNums\", \"outNums_mean_s_w_h_%s\"%i)\n",
    "        df_label=feat_std(df_label,df_select,[\"stationID\", 'weekday','hour'],\"outNums\", \"outNums_std_s_w_h_%s\"%i)\n",
    "        df_label=feat_median(df_label,df_select,[\"stationID\", 'weekday','hour'],\"outNums\", \"outNums_median_s_w_h_%s\"%i)\n",
    "        df_label=feat_max(df_label,df_select,[\"stationID\", 'weekday','hour'],\"outNums\", \"outNums_max_s_w_h_%s\"%i)\n",
    "        df_label=feat_min(df_label,df_select,[\"stationID\", 'weekday','hour'],\"outNums\", \"outNums_min_s_w_h_%s\"%i)\n",
    "        df_label=feat_var(df_label,df_select,[\"stationID\", 'weekday','hour'],\"outNums\", \"outNums_var_s_w_h_%s\"%i)\n",
    "        #f_label=feat_quantile(df_label,df_select,[\"stationID\", 'weekday','hour'],\"outNums\", \"outNums_quantile_s_w_h_%s\"%i)\n",
    "\n",
    "        df_label=feat_mean(df_label,df_select,[\"stationID\", 'weekday','hour','minute'],\"inNums\", \"inNums_mean_s_w_hm_%s\"%i)\n",
    "        df_label=feat_std(df_label,df_select,[\"stationID\", 'weekday','hour','minute'],\"inNums\", \"inNums_std_s_w_hm_%s\"%i)\n",
    "        df_label=feat_median(df_label,df_select,[\"stationID\", 'weekday','hour','minute'],\"inNums\", \"inNums_median_s_w_hm_%s\"%i)\n",
    "        df_label=feat_max(df_label,df_select,[\"stationID\", 'weekday','hour','minute'],\"inNums\", \"inNums_max_s_w_hm_%s\"%i)\n",
    "        df_label=feat_min(df_label,df_select,[\"stationID\", 'weekday','hour','minute'],\"inNums\", \"inNums_min_s_w_hm_%s\"%i)\n",
    "        df_label=feat_var(df_label,df_select,[\"stationID\", 'weekday','hour','minute'],\"inNums\", \"inNums_var_s_w_hm_%s\"%i)\n",
    "\n",
    "        df_label=feat_mean(df_label,df_select,[\"stationID\", 'weekday','hour','minute'],\"outNums\", \"outNums_mean_s_w_hm_%s\"%i)\n",
    "        df_label=feat_std(df_label,df_select,[\"stationID\", 'weekday','hour','minute'],\"outNums\", \"outNums_std_s_w_hm_%s\"%i)\n",
    "        df_label=feat_median(df_label,df_select,[\"stationID\", 'weekday','hour','minute'],\"outNums\", \"outNums_median_s_w_hm_%s\"%i)\n",
    "        df_label=feat_max(df_label,df_select,[\"stationID\", 'weekday','hour','minute'],\"outNums\", \"outNums_max_s_w_hm_%s\"%i)\n",
    "        df_label=feat_min(df_label,df_select,[\"stationID\", 'weekday','hour','minute'],\"outNums\", \"outNums_min_s_w_hm_%s\"%i)\n",
    "        df_label=feat_var(df_label,df_select,[\"stationID\", 'weekday','hour','minute'],\"outNums\", \"outNums_var_s_w_hm_%s\"%i)\n",
    "    return df_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5e6a15ce28a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192586, 19)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the slip is: 2\n",
      "19\n",
      "17\n",
      "15\n",
      "13\n",
      "11\n",
      "9\n",
      "7\n",
      "5\n",
      "3\n",
      "21\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[500]\ttraining's l1: 11.4042\tvalid_1's l1: 14.0314\n",
      "[1000]\ttraining's l1: 9.79313\tvalid_1's l1: 13.7989\n",
      "[1500]\ttraining's l1: 8.67525\tvalid_1's l1: 13.713\n",
      "[2000]\ttraining's l1: 7.80314\tvalid_1's l1: 13.6752\n",
      "[2500]\ttraining's l1: 7.0871\tvalid_1's l1: 13.6671\n",
      "Early stopping, best iteration is:\n",
      "[2388]\ttraining's l1: 7.23886\tvalid_1's l1: 13.6611\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[500]\ttraining's l1: 11.4225\tvalid_1's l1: 13.8004\n",
      "[1000]\ttraining's l1: 9.83096\tvalid_1's l1: 13.6253\n"
     ]
    }
   ],
   "source": [
    "df_train = data\n",
    "# 根据划窗计算id&num特征\n",
    "# 按出入车站group，看两站进出时间的统计特征\n",
    "for slip in [2,3,5,7]:\n",
    "    print(f'the slip is: {slip}')\n",
    "    t_end = 21\n",
    "    nday = slip\n",
    "\n",
    "    # 构造训练集\n",
    "    all_data = []\n",
    "    for i in range(nday*1, nday*(19//nday+1),nday):\n",
    "        t_begin = t_end-i\n",
    "        print(t_begin)\n",
    "        df_train[\"day_gap\"]=df_train[\"day\"].apply(lambda x:int(x-t_begin))\n",
    "        df_feature=df_train[df_train.day_gap<0].copy()\n",
    "        df_label=df_train[(df_train.day_gap>=0)&(df_train.day_gap<nday)][[\"stationID\",\"startTime\",'weekday','hour',\n",
    "        'minute','inNums','outNums']].copy()\n",
    "        train_data_tmp=create_features(df_label,df_feature)\n",
    "        all_data.append(train_data_tmp)\n",
    "    train=pd.concat(all_data)\n",
    "    #构造线上测试集\n",
    "    t_begin=21\n",
    "    print(t_begin)\n",
    "    df_label=df_train.loc[df_train['day']==21, ['stationID','startTime','weekday','hour','minute','inNums','outNums']]\n",
    "    df_label[\"day_gap\"]=0\n",
    "    df_train[\"day_gap\"]=df_train[\"day\"].apply(lambda x:int(x-t_begin))\n",
    "    df_label=df_label[['stationID','startTime','weekday','hour','minute','inNums','outNums']].copy()\n",
    "    test=create_features(df_label,df_train)\n",
    "\n",
    "    #save features data for stacking\n",
    "    #train.to_csv(\"../stacking/train.csv\",index=None)\n",
    "    #test.to_csv(\"../stacking/test.csv\",index=None)\n",
    "\n",
    "    #训练预测\n",
    "    #weight_df=train[[\"day_gap\"]].copy()\n",
    "    #weight_df[\"weight\"]=weight_df[\"day_gap\"].apply(lambda x: 1 if x<=6 else 1)\n",
    "    def stacking(reg, train_data, test_data, reg_name, inOrout, params):\n",
    "        train_pre = np.zeros(train_data.shape[0])\n",
    "        test_pre = np.zeros(test_data.shape[0])\n",
    "        cv_score = []\n",
    "        \n",
    "        all_cols = [col for col in train_data.columns if col not in ['inNums', 'outNums']]\n",
    "        train_x = train_data[all_cols].values\n",
    "        train_y = train[inOrout].values\n",
    "        test_data = test_data[all_cols].values\n",
    "        for i, (trn_index, val_index) in enumerate(kf.split(train_data)):\n",
    "            trn_x = train_x[trn_index]\n",
    "            trn_y = train_y[trn_index]\n",
    "            \n",
    "            val_x = train_x[val_index]\n",
    "            val_y = train_y[val_index]\n",
    "            #weight_train=weight_df.iloc[trn_index]\n",
    "            #weight_test=weight_df.iloc[val_index]\n",
    "\n",
    "            trn_matrix = reg.Dataset(trn_x, label=trn_y)\n",
    "            val_matrix = reg.Dataset(val_x, label=val_y)\n",
    "            num_round = 200000\n",
    "            early_stopping_rounds = 500\n",
    "            if val_matrix:\n",
    "                model = reg.train(params, trn_matrix, num_round, valid_sets=[trn_matrix, val_matrix],\n",
    "                                  early_stopping_rounds=early_stopping_rounds, verbose_eval=500\n",
    "                                  )\n",
    "                pre= model.predict(val_x,num_iteration=model.best_iteration)\n",
    "                train_pre[val_index]=pre\n",
    "                test_pre += (model.predict(test_data, num_iteration=model.best_iteration)) / folds\n",
    "                cv_score.append(mean_absolute_error(val_y, pre))\n",
    "\n",
    "            #print(f\"folds {i} of {reg_name} score is: {mean_absolute_error(val_y, pre)}\")\n",
    "            \n",
    "        print(\"%s_score_list:\"%reg_name,cv_score)\n",
    "        print(\"%s_score_mean:\"%reg_name,np.mean(cv_score))\n",
    "\n",
    "        return train_pre.reshape(-1,1), test_pre.reshape(-1,1), np.mean(cv_score)\n",
    "\n",
    "    def lgb_reg(train, test):\n",
    "        params = {\n",
    "            'boosting': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mae',\n",
    "            'num_leaves': 63,\n",
    "            'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.9,\n",
    "            'bagging_seed':0,\n",
    "            'bagging_freq': 1,\n",
    "            'reg_alpha':1,\n",
    "            'reg_lambda':2,\n",
    "            'verbose':500,\n",
    "            'num_threads': 4\n",
    "        }\n",
    "        lgb_train_in, lgb_test_in, cv_scores_in = stacking(lgb,train,test,\"lgb\", 'inNums', params)\n",
    "        lgb_train_out, lgb_test_out, cv_scores_out = stacking(lgb,train,test,\"lgb\", 'outNums', params)\n",
    "        return lgb_train_in, lgb_test_in, cv_scores_in, lgb_train_out, lgb_test_out, cv_scores_out\n",
    "\n",
    "    import lightgbm as lgb\n",
    "    folds = 10\n",
    "    seed = 2019\n",
    "\n",
    "    #生成数据\n",
    "    # 考虑不去stationID\n",
    "\n",
    "    train_data = train.drop(columns=['stationID','startTime'])\n",
    "    test_data = test.drop(columns=['stationID','startTime'])\n",
    "\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    lgb_train_in, lgb_test_in, cv_scores_in, lgb_train_out, lgb_test_out, cv_scores_out=lgb_reg(train_data,test_data)\n",
    "\n",
    "    #生成线下\n",
    "    train[\"inNums_pre\"]=np.clip(lgb_train_in,0,5000)\n",
    "    train[\"outNums_pre\"]=np.clip(lgb_train_out,0,5000)\n",
    "    score_result=mean_absolute_error(train[\"inNums_pre\"], train[\"inNums\"]) + mean_absolute_error(train[\"outNums_pre\"], train[\"outNums\"]) \n",
    "    print(f'slip {slip}: the total mae score is {score_result/2}')\n",
    "    #生成提交\n",
    "    test_name = os.path.join(TEST_A_PATH, 'testA_submit_2019-01-29.csv')\n",
    "    submit = pd.read_csv(test_name)\n",
    "    \n",
    "    #lgb_test_in = np.round(lgb_test_in)\n",
    "    #lgb_test_out = np.round(lgb_test_out)\n",
    "    submit['inNums'] = np.clip(lgb_test_in,0,5000)\n",
    "    submit['outNums'] = np.clip(lgb_test_out,0,5000)\n",
    "    submit.to_csv(f'../submit/{slip}slips_{folds}folds.csv', index=False)\n",
    "    \n",
    "test_name = os.path.join(TEST_A_PATH, 'testA_submit_2019-01-29.csv')\n",
    "submit = pd.read_csv(test_name)\n",
    "i = 0\n",
    "for file in os.listdir('../submit'):\n",
    "    print(f'the file: {file}')\n",
    "    temp = pd.read_csv('../submit/'+file)\n",
    "    if i==0:\n",
    "        submit['inNums'] = temp['inNums']\n",
    "        submit['outNums'] = temp['outNums']\n",
    "        i = 1\n",
    "    else:\n",
    "        submit['inNums'] += temp['inNums']\n",
    "        submit['outNums'] += temp['outNums']\n",
    "files = [file for file in os.listdir('../submit') if file[-4:]=='.csv']\n",
    "len_files = len(files)\n",
    "submit['inNums'] = submit['inNums'].apply(lambda x: x/len_files)\n",
    "submit['outNums'] = submit['outNums'].apply(lambda x: x/len_files)\n",
    "submit.to_csv(f'../submit/submit_{datetime.strftime(datetime.now(), \"%Y_%m_%d_%H_%M\")}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 15.1\n",
    "# 2 14.2\n",
    "# 3 14.1\n",
    "# 5 14.0\n",
    "# 7 14.9\n",
    "# 10 15.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the file: 2slips_10folds.csv\n",
      "the file: 3slips_10folds.csv\n",
      "the file: 5slips_10folds.csv\n",
      "the file: 7slips_10folds.csv\n"
     ]
    }
   ],
   "source": [
    "test_name = os.path.join(TEST_A_PATH, 'testA_submit_2019-01-29.csv')\n",
    "submit = pd.read_csv(test_name)\n",
    "i = 0\n",
    "for file in os.listdir('../submit'):\n",
    "    print(f'the file: {file}')\n",
    "    temp = pd.read_csv('../submit/'+file)\n",
    "    if i==0:\n",
    "        submit['inNums'] = temp['inNums']\n",
    "        submit['outNums'] = temp['outNums']\n",
    "        i = 1\n",
    "    else:\n",
    "        submit['inNums'] += temp['inNums']\n",
    "        submit['outNums'] += temp['outNums']\n",
    "files = [file for file in os.listdir('../submit') if file[-4:]=='.csv']\n",
    "len_files = len(files)\n",
    "submit['inNums'] = submit['inNums'].apply(lambda x: x/len_files)\n",
    "submit['outNums'] = submit['outNums'].apply(lambda x: x/len_files)\n",
    "submit.to_csv(f'../submit/submit_{datetime.strftime(datetime.now(), \"%Y_%m_%d_%H_%M\")}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对夜晚数据单独处理\n",
    "data = pd.read_csv('../input/after_base_features.csv')\n",
    "data['hour_minutes'] = data['hour']*60+data['minute']\n",
    "data_in = data.loc[(data['hour_minutes']>=1420) | (data['hour_minutes']<=320), ['stationID','day','hour_minutes','inNums','outNums']]\n",
    "data_out = data.loc[data['hour_minutes']<=350, ['stationID','day','hour_minutes','inNums','outNums']]\n",
    "data_in = data_in[data_in['inNums'] != 0]\n",
    "data_out = data_out[data_out['outNums'] != 0]\n",
    "\n",
    "def special_time(df_data, df_label, inOrout):\n",
    "    # count\n",
    "    tmp = df_data.groupby(['stationID', 'hour_minutes'])['day'].count().reset_index(name=f'count_inNum_days_{inOrout}')\n",
    "    df_label = df_label.merge(tmp, on=['stationID', 'hour_minutes'], how='left')\n",
    "    #submit.fillnan(0)\n",
    "    tmp = df_data.groupby(['stationID', 'hour_minutes'])['day'].count().reset_index(name=f'count_outNum_days_{inOrout}')\n",
    "    df_label = df_label.merge(tmp, on=['stationID', 'hour_minutes'], how='left')\n",
    "    # mean\n",
    "    tmp = df_data.groupby(['stationID', 'hour_minutes'])['inNums'].mean().reset_index(name=f'mean_inNums_{inOrout}')\n",
    "    df_label = df_label.merge(tmp, on=['stationID', 'hour_minutes'], how='left')\n",
    "    tmp = df_data.groupby(['stationID', 'hour_minutes'])['outNums'].mean().reset_index(name=f'mean_outNums_{inOrout}')\n",
    "    df_label = df_label.merge(tmp, on=['stationID', 'hour_minutes'], how='left')\n",
    "    # mode\n",
    "\n",
    "    return df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stationID</th>\n",
       "      <th>startTime</th>\n",
       "      <th>endTime</th>\n",
       "      <th>inNums</th>\n",
       "      <th>outNums</th>\n",
       "      <th>hour_x</th>\n",
       "      <th>minute_x</th>\n",
       "      <th>hour_minutes</th>\n",
       "      <th>hour_y</th>\n",
       "      <th>minute_y</th>\n",
       "      <th>count_inNum_days_in</th>\n",
       "      <th>count_outNum_days_in</th>\n",
       "      <th>mean_inNums_in</th>\n",
       "      <th>mean_outNums_in</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>count_inNum_days_out</th>\n",
       "      <th>count_outNum_days_out</th>\n",
       "      <th>mean_inNums_out</th>\n",
       "      <th>mean_outNums_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-29 00:00:00</td>\n",
       "      <td>2019-01-29 00:10:00</td>\n",
       "      <td>0.724791</td>\n",
       "      <td>1.574449</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-29 00:10:00</td>\n",
       "      <td>2019-01-29 00:20:00</td>\n",
       "      <td>0.799236</td>\n",
       "      <td>1.370402</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-29 00:20:00</td>\n",
       "      <td>2019-01-29 00:30:00</td>\n",
       "      <td>1.210040</td>\n",
       "      <td>1.301620</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-29 00:30:00</td>\n",
       "      <td>2019-01-29 00:40:00</td>\n",
       "      <td>1.606611</td>\n",
       "      <td>1.232190</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-29 00:40:00</td>\n",
       "      <td>2019-01-29 00:50:00</td>\n",
       "      <td>1.827594</td>\n",
       "      <td>1.220876</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stationID            startTime              endTime    inNums   outNums  \\\n",
       "0          0  2019-01-29 00:00:00  2019-01-29 00:10:00  0.724791  1.574449   \n",
       "1          0  2019-01-29 00:10:00  2019-01-29 00:20:00  0.799236  1.370402   \n",
       "2          0  2019-01-29 00:20:00  2019-01-29 00:30:00  1.210040  1.301620   \n",
       "3          0  2019-01-29 00:30:00  2019-01-29 00:40:00  1.606611  1.232190   \n",
       "4          0  2019-01-29 00:40:00  2019-01-29 00:50:00  1.827594  1.220876   \n",
       "\n",
       "   hour_x  minute_x  hour_minutes  hour_y  minute_y  count_inNum_days_in  \\\n",
       "0       0         0             0     0.0       0.0                  0.0   \n",
       "1       0        10            10     0.0      10.0                  1.0   \n",
       "2       0        20            20     0.0      20.0                  1.0   \n",
       "3       0        30            30     0.0      30.0                  3.0   \n",
       "4       0        40            40     0.0      40.0                  1.0   \n",
       "\n",
       "   count_outNum_days_in  mean_inNums_in  mean_outNums_in  hour  minute  \\\n",
       "0                   0.0        0.000000         0.000000   0.0     0.0   \n",
       "1                   1.0        2.000000         0.000000   0.0    10.0   \n",
       "2                   1.0        1.000000         2.000000   0.0    20.0   \n",
       "3                   3.0        1.333333         0.333333   0.0    30.0   \n",
       "4                   1.0        1.000000         1.000000   0.0    40.0   \n",
       "\n",
       "   count_inNum_days_out  count_outNum_days_out  mean_inNums_out  \\\n",
       "0                   7.0                    7.0             0.00   \n",
       "1                   4.0                    4.0             0.00   \n",
       "2                   4.0                    4.0             0.25   \n",
       "3                   2.0                    2.0             0.50   \n",
       "4                   4.0                    4.0             0.25   \n",
       "\n",
       "   mean_outNums_out  \n",
       "0          2.142857  \n",
       "1          1.750000  \n",
       "2          2.000000  \n",
       "3          1.000000  \n",
       "4          1.250000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../submit/submit_2019_03_28_09_10.csv')\n",
    "submit['hour'] = submit['startTime'].apply(lambda x: int(str(x)[11:13]))\n",
    "submit['minute'] = submit['startTime'].apply(lambda x: int(str(x)[14:15]+'0'))# hour+10min 10min最后可以删除\n",
    "submit['hour_minutes'] = submit['hour']*60+submit['minute']\n",
    "\n",
    "submit_in = special_time(data_in, submit[(submit['hour_minutes']>=1420) | (submit['hour_minutes']<=320)], 'in')\n",
    "submit_out = special_time(data_out, submit[submit['hour_minutes']<=350], 'out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_in = submit_in.fillna(0)\n",
    "submit_out = submit_out.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = submit.merge(submit_in, on=['stationID','hour_minutes','startTime','endTime','inNums','outNums'], how='left')\n",
    "submit = submit.merge(submit_out, on=['stationID','hour_minutes','startTime','endTime','inNums','outNums'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.loc[(submit['hour_minutes']>=1420) | (submit['hour_minutes']<=320), 'inNums'] = submit.loc[(submit['hour_minutes']>=1420)| (submit['hour_minutes']<=320), 'mean_inNums_in']\n",
    "submit.loc[submit['hour_minutes']<=350, 'outNums'] = submit.loc[submit['hour_minutes']<=350, 'mean_outNums_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit[['stationID', 'startTime', 'endTime', 'inNums', 'outNums']].to_csv(f'../submit/submit_{datetime.strftime(datetime.now(), \"%Y_%m_%d_%H_%M\")}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../submit/submit_2019_03_28_09_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11664, 5)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
