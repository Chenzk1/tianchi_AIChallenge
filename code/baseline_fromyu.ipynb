{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from scipy.stats import norm, rankdata\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set(font_scale=1)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(props):\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if props[col].dtype != object:  # Exclude strings\n",
    "            \n",
    "            # Print current column type\n",
    "            print(\"******************************\")\n",
    "            print(\"Column: \",col)\n",
    "            print(\"dtype before: \",props[col].dtype)\n",
    "            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(props[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = props[col].fillna(0).astype(np.int64)\n",
    "            result = (props[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "            \n",
    "            # Print new column type\n",
    "            print(\"dtype after: \",props[col].dtype)\n",
    "            print(\"******************************\")\n",
    "    \n",
    "    # Print final result\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return props\n",
    "def read_data(name, **params):\n",
    "    data = pd.read_csv(name, **params)\n",
    "    data = reduce_mem_usage(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 128.43527221679688  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  66.51115989685059  MB\n",
      "This is  51.785742926273954 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "path = '../input/RAW_DATA'\n",
    "test = pd.read_csv(path + '/Metro_testA/testA_submit_2019-01-29.csv')\n",
    "test_28 = read_data(path + '/Metro_testA/testA_record_2019-01-28.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_features(df_):\n",
    "    '''\n",
    "    day week weekend hour minute\n",
    "    '''\n",
    "    df = df_.copy()\n",
    "    \n",
    "    # base time\n",
    "    df['day']     = df['time'].apply(lambda x: int(x[8:10]))\n",
    "    df['week']    = pd.to_datetime(df['time']).dt.dayofweek + 1\n",
    "    df['weekend'] = (pd.to_datetime(df.time).dt.weekday >=5).astype(int)\n",
    "    df['hour']    = df['time'].apply(lambda x: int(x[11:13]))\n",
    "    df['minute']  = df['time'].apply(lambda x: int(x[14:15]+'0'))\n",
    "    \n",
    "    # count,sum\n",
    "    result = df.groupby(['stationID', 'week', 'weekend', 'day', 'hour', 'minute']).status.agg(['count', 'sum']).reset_index()\n",
    "    \n",
    "    # nunique\n",
    "    tmp     = df.groupby(['stationID'])['deviceID'].nunique().reset_index(name='nuni_deviceID_of_stationID')\n",
    "    result  = result.merge(tmp, on=['stationID'], how='left')\n",
    "    tmp     = df.groupby(['stationID','hour'])['deviceID'].nunique().reset_index(name='nuni_deviceID_of_stationID_hour')\n",
    "    result  = result.merge(tmp, on=['stationID','hour'], how='left')\n",
    "    tmp     = df.groupby(['stationID','hour','minute'])['deviceID'].nunique().\\\n",
    "                                           reset_index(name='nuni_deviceID_of_stationID_hour_minute')\n",
    "    result  = result.merge(tmp, on=['stationID','hour','minute'], how='left')\n",
    "    \n",
    "    # in,out\n",
    "    result['inNums']  = result['sum']\n",
    "    result['outNums'] = result['count'] - result['sum']\n",
    "    \n",
    "    #\n",
    "    result['day_since_first'] = result['day'] - 1 \n",
    "    result.fillna(0, inplace=True)\n",
    "    del result['sum'],result['count']\n",
    "    \n",
    "    return result\n",
    "data = get_base_features(test_28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_2019-01-01.csv 0\n",
      "Memory usage of properties dataframe is : 135.62892150878906  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  70.23644256591797  MB\n",
      "This is  51.78574140720163 % of the initial size\n",
      "record_2019-01-02.csv 1\n",
      "Memory usage of properties dataframe is : 126.91683959960938  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  65.72482872009277  MB\n",
      "This is  51.78574326892951 % of the initial size\n",
      "record_2019-01-03.csv 2\n",
      "Memory usage of properties dataframe is : 122.4846420288086  MB\n",
      "******************************\n",
      "Column:  stationID\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  deviceID\n",
      "dtype before:  int64\n",
      "dtype after:  uint16\n",
      "******************************\n",
      "******************************\n",
      "Column:  status\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "******************************\n",
      "Column:  payType\n",
      "dtype before:  int64\n",
      "dtype after:  uint8\n",
      "******************************\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  63.42958354949951  MB\n",
      "This is  51.785744317708634 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "data_list = os.listdir(path+'/Metro_train/')\n",
    "a = 0\n",
    "for i in range(0, len(data_list)):\n",
    "    a+=1\n",
    "    if data_list[i].split('.')[-1] == 'csv':\n",
    "        print(data_list[i], i)\n",
    "        df = read_data(path+'/Metro_train/' + data_list[i])\n",
    "        df = get_base_features(df)\n",
    "        data = pd.concat([data, df], axis=0, ignore_index=True)\n",
    "        if a==3:\n",
    "            break\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剔除周末,并修改为连续时间\n",
    "data = data[(data.day!=5)&(data.day!=6)]\n",
    "data = data[(data.day!=12)&(data.day!=13)]\n",
    "data = data[(data.day!=19)&(data.day!=20)]\n",
    "data = data[(data.day!=26)&(data.day!=27)]\n",
    "\n",
    "def fix_day(d):\n",
    "    if d in [1,2,3,4]:\n",
    "        return d\n",
    "    elif d in [7,8,9,10,11]:\n",
    "        return d - 2\n",
    "    elif d in [14,15,16,17,18]:\n",
    "        return d - 4\n",
    "    elif d in [21,22,23,24,25]:\n",
    "        return d - 6\n",
    "    elif d in [28]:\n",
    "        return d - 8\n",
    "data['day'] = data['day'].apply(fix_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20    9124\n",
       "2     9046\n",
       "3     9028\n",
       "1     8941\n",
       "Name: day, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.day.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将test拼接到训练数据中\n",
    "# 有一些列无法产生，大概用了nan\n",
    "test['week']    = pd.to_datetime(test['startTime']).dt.dayofweek + 1\n",
    "test['weekend'] = (pd.to_datetime(test.startTime).dt.weekday >=5).astype(int)\n",
    "test['day']     = test['startTime'].apply(lambda x: int(x[8:10]))\n",
    "test['hour']    = test['startTime'].apply(lambda x: int(x[11:13]))\n",
    "test['minute']  = test['startTime'].apply(lambda x: int(x[14:15]+'0'))\n",
    "test['day_since_first'] = test['day'] - 1\n",
    "test = test.drop(['startTime','endTime'], axis=1)\n",
    "data = pd.concat([data,test], axis=0, ignore_index=True)\n",
    "\n",
    "stat_columns = ['inNums','outNums']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "day                                           0\n",
       "day_since_first                               0\n",
       "hour                                          0\n",
       "inNums                                    11664\n",
       "minute                                        0\n",
       "nuni_deviceID_of_stationID                11664\n",
       "nuni_deviceID_of_stationID_hour           11664\n",
       "nuni_deviceID_of_stationID_hour_minute    11664\n",
       "outNums                                   11664\n",
       "stationID                                     0\n",
       "week                                          0\n",
       "weekend                                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refer_day(d):\n",
    "    if d == 20:\n",
    "        return 29\n",
    "    else:\n",
    "        return d + 1\n",
    "# 1->1 1->2 2->3...20->29\n",
    "tmp = data.copy()\n",
    "tmp_df = tmp[tmp.day==1]\n",
    "tmp_df['day'] = tmp_df['day'] - 1\n",
    "tmp = pd.concat([tmp, tmp_df], axis=0, ignore_index=True)\n",
    "tmp['day'] = tmp['day'].apply(get_refer_day)\n",
    "\n",
    "for f in stat_columns:\n",
    "    tmp.rename(columns={f: f+'_last'}, inplace=True) \n",
    "    \n",
    "tmp = tmp[['stationID','day','hour','minute','inNums_last','outNums_last']]\n",
    "# 相当于把前一天的innum和outnum加在了当天行\n",
    "# 但是要求比较严格 必须是同十分钟内的\n",
    "data = data.merge(tmp, on=['stationID','day','hour','minute'], how='left')\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29    11664\n",
       "20     9124\n",
       "2      9046\n",
       "3      9028\n",
       "1      8941\n",
       "Name: day, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.day.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    11664\n",
       "Name: inNums, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['day']==29,'inNums'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data.groupby(['stationID','week','hour','minute'], as_index=False)['inNums'].agg({\n",
    "                                                                        'inNums_whm_max'    : 'max',\n",
    "                                                                        'inNums_whm_min'    : 'min',\n",
    "                                                                        'inNums_whm_mean'   : 'mean'\n",
    "                                                                        })\n",
    "data = data.merge(tmp, on=['stationID','week','hour','minute'], how='left')\n",
    "\n",
    "tmp = data.groupby(['stationID','week','hour','minute'], as_index=False)['outNums'].agg({\n",
    "                                                                        'outNums_whm_max'    : 'max',\n",
    "                                                                        'outNums_whm_min'    : 'min',\n",
    "                                                                        'outNums_whm_mean'   : 'mean'\n",
    "                                                                        })\n",
    "data = data.merge(tmp, on=['stationID','week','hour','minute'], how='left')\n",
    "\n",
    "tmp = data.groupby(['stationID','week','hour'], as_index=False)['inNums'].agg({\n",
    "                                                                        'inNums_wh_max'    : 'max',\n",
    "                                                                        'inNums_wh_min'    : 'min',\n",
    "                                                                        'inNums_wh_mean'   : 'mean'\n",
    "                                                                        })\n",
    "data = data.merge(tmp, on=['stationID','week','hour'], how='left')\n",
    "\n",
    "tmp = data.groupby(['stationID','week','hour'], as_index=False)['outNums'].agg({\n",
    "                                                                        #'outNums_wh_max'    : 'max',\n",
    "                                                                        #'outNums_wh_min'    : 'min',\n",
    "                                                                        'outNums_wh_mean'   : 'mean'\n",
    "                                                                        })\n",
    "data = data.merge(tmp, on=['stationID','week','hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0       2226\n",
       "1.0        186\n",
       "2.0        186\n",
       "101.0       90\n",
       "4.0         90\n",
       "87.0        78\n",
       "13.0        72\n",
       "60.0        72\n",
       "11.0        72\n",
       "3.0         72\n",
       "77.0        72\n",
       "7.0         66\n",
       "113.0       66\n",
       "150.0       66\n",
       "5.0         66\n",
       "95.0        66\n",
       "99.0        60\n",
       "55.0        60\n",
       "57.0        60\n",
       "33.0        60\n",
       "51.0        60\n",
       "131.0       60\n",
       "15.0        60\n",
       "42.0        60\n",
       "54.0        60\n",
       "117.0       60\n",
       "166.0       60\n",
       "97.0        60\n",
       "94.0        60\n",
       "64.0        54\n",
       "          ... \n",
       "247.0        6\n",
       "410.0        6\n",
       "235.0        6\n",
       "193.0        6\n",
       "277.0        6\n",
       "1105.0       6\n",
       "612.0        6\n",
       "279.0        6\n",
       "755.0        6\n",
       "542.0        6\n",
       "2725.0       6\n",
       "129.0        6\n",
       "259.0        6\n",
       "532.0        6\n",
       "265.0        6\n",
       "1934.0       6\n",
       "267.0        6\n",
       "669.0        6\n",
       "382.0        6\n",
       "362.0        6\n",
       "477.0        6\n",
       "1393.0       6\n",
       "473.0        6\n",
       "953.0        6\n",
       "688.0        6\n",
       "426.0        6\n",
       "654.0        6\n",
       "846.0        6\n",
       "3039.0       6\n",
       "967.0        6\n",
       "Name: inNums_wh_max, Length: 403, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['day']==29,'inNums_wh_max'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_day(d):\n",
    "    if d in [1,2,3,4]:\n",
    "        return d\n",
    "    elif d in [5,6,7,8,9]:\n",
    "        return d + 2\n",
    "    elif d in [10,11,12,13,14]:\n",
    "        return d + 4\n",
    "    elif d in [15,16,17,18,19]:\n",
    "        return d + 6\n",
    "    elif d == 20:\n",
    "        return d + 8\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "all_columns = [f for f in data.columns if f not in ['weekend','inNums','outNums']]\n",
    "### all data\n",
    "all_data = data[data.day!=29]\n",
    "all_data['day'] = all_data['day'].apply(recover_day)\n",
    "X_data = all_data[all_columns].values\n",
    "\n",
    "train = data[data.day <20]\n",
    "train['day'] = train['day'].apply(recover_day)\n",
    "X_train = train[all_columns].values\n",
    "\n",
    "valid = data[data.day==20]\n",
    "valid['day'] = valid['day'].apply(recover_day)\n",
    "X_valid = valid[all_columns].values\n",
    "\n",
    "test  = data[data.day==29]\n",
    "X_test = test[all_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'num_leaves': 63,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_seed':0,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': 1,\n",
    "    'reg_alpha':1,\n",
    "    'reg_lambda':2\n",
    "}\n",
    "\n",
    "######################################################inNums\n",
    "y_train = train['inNums']\n",
    "y_valid = valid['inNums']\n",
    "y_data  = all_data['inNums']\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_evals = lgb.Dataset(X_valid, y_valid , reference=lgb_train)\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10000,\n",
    "                valid_sets=[lgb_train,lgb_evals],\n",
    "                valid_names=['train','valid'],\n",
    "                early_stopping_rounds=200,\n",
    "                verbose_eval=1000,\n",
    "                )\n",
    "\n",
    "### all_data\n",
    "lgb_train = lgb.Dataset(X_data, y_data)\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=gbm.best_iteration,\n",
    "                valid_sets=[lgb_train],\n",
    "                valid_names=['train'],\n",
    "                verbose_eval=1000,\n",
    "                )\n",
    "test['inNums'] = gbm.predict(X_test)\n",
    "\n",
    "######################################################outNums\n",
    "y_train = train['outNums']\n",
    "y_valid = valid['outNums']\n",
    "y_data  = all_data['outNums']\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_evals = lgb.Dataset(X_valid, y_valid , reference=lgb_train)\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10000,\n",
    "                valid_sets=[lgb_train,lgb_evals],\n",
    "                valid_names=['train','valid'],\n",
    "                early_stopping_rounds=200,\n",
    "                verbose_eval=1000,\n",
    "                )\n",
    "\n",
    "### all_data\n",
    "lgb_train = lgb.Dataset(X_data, y_data)\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=gbm.best_iteration,\n",
    "                valid_sets=[lgb_train],\n",
    "                valid_names=['train'],\n",
    "                verbose_eval=1000,\n",
    "                )\n",
    "test['outNums'] = gbm.predict(X_test)\n",
    "\n",
    "sub = pd.read_csv(path + '/Metro_testA/testA_submit_2019-01-29.csv')\n",
    "sub['inNums']   = test['inNums'].values\n",
    "sub['outNums']  = test['outNums'].values\n",
    "# 结果修正\n",
    "sub.loc[sub.inNums<0 , 'inNums']  = 0\n",
    "sub.loc[sub.outNums<0, 'outNums'] = 0\n",
    "sub[['stationID', 'startTime', 'endTime', 'inNums', 'outNums']].to_csv('output/sub_model.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
